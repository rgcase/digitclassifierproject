<!DOCTYPE html>
<html lang="en">
<head>
    <title>Digit Classifier</title>
    <link rel="stylesheet" href="./css/bootstrap.min.css">
</head>

<body>
<div class="navbar navbar-default navbar-fixed-top">
    <div class="container">
        <div class="navbar-header">
            <div class="navbar-brand">Digit Classifier</div>
        </div>
    </div>
</div>

<div class="container">
    <div class="page-header" id="banner">
        <div class="row">
            <div class="col-lg-8 col-md-7 col-sm-6">
                <h1>CISC 859 Digit Classifier Project</h1></br>
                <h3>Ryan Case, 10071517</h3>
            </div>
        </div>
    </div>
</div>

<div class="container">
    <div class="row">
        <div class="col-lg-8">
            <h1>Introduction</h1>
            <p>
            This is a classifier for numerical digits 0-9 that was completed as a project 
            for CISC 859 at Queen's University.
            </p>
            <p>
            Image files are accepted in .bmp format for both training and classification,
            and are dealt with using the Pillow[1] library.
            </p>
            <p>
            Classification methods include a naive implementation of k-nearest neighbours,
            and multiple other methods that are available in the scikit-learn[2] library.
            </p>
            <p>
            The accuracy of the classifier is not really acceptable for any real use but 
            it does a better job than random guessing. The framework is here though, so 
            it could be improved by doing better feature selection and tweaking the 
            classifiers that are used.
            </p>
            <p>
            The source code (along with this website) are hosted on github. You can find it
            <a href="http://github.com/rgcase/digitclassifierproject">here</a>.
            </p>
        </div>
    </div>
</div>

<hr>

<div class="container">
    <div class="row">
        <div class="col-lg-8">
            <h1>Feature Extraction</h1>
            <p>
            The examples I used for both the training and test data were obtained from
            the CISC 859 course website. These images can be found 
            <a href="http://research.cs.queensu.ca/home/cisc859/download/859.digits/bmp.images.patrice/">
            here</a>. The images used for training were the files prefixed by "trg" and 
            the test images were the files prefixed by "test".
            </p>
            <p>
            The first phase of extracting features is to extract the digits themselves
            from the .bmp images. This happens in <a href="https://github.com/rgcase/digitclassifierproject/blob/master/partitionimages.py">
            partitionimages.py</a>. Each image file is opened and loaded as a greyscale 
            Image object using Pillow.
            </p>
            <p>
            Each file is thresholded so that pixels with value less than 200 are set to
            0 and pixels with value greater than 200 are set to 255. After several 
            tests, this threshold value seemed to produce the best results when 
            extracting the individual digits. This extraction is done using a region
            growing algorithm that can be found in 
            <a href="https://github.com/rgcase/digitclassifierproject/blob/master/imageprocessing.py">
            imageprocessing.py</a>. Once a black region is found, the bounding box
            is computed, then a new Image object is created containing everything
            in that bounding box. The image is saved in a folder containing all the 
            images of the same digit, separated into training and test set.
            </p>
            <p>
            Once the digits have all been extracted, the feature extraction can begin. 
            All of this is accomplished in the 
            <a href="https://github.com/rgcase/digitclassifierproject/blob/master/extractfeatures.py">
            extractfeatures.py</a> file. The features that I used are:
            </p>
            <ul>
                <li>Blackness Ratio</li>
                <li>Aspect Ratio of the Bounding Box</li>
                <li>Number of Holes</li>
                <li>Half-Blackness Ratio Top, Bottom, Left, Right)</li>
                <li>Ratio of Longest Horizontal Line to Horizontal Width</li>
                <li>Ratio of Longest Vertical Line to Vertical Width</li>
            </ul>
            <p>
            All this information is written to a .csv file so it can be used later
            by the classification algorithms.
            </p>
            <p>
            In the following table, for each digit the values of each measured 
            feature are shown rounded to one decimal place, with the percentage 
            of that digit having the corresponding feature value next to it. The 
            table includes the data from the entire training set.
            </p>
        </div>
    </div>
    
    <div class="bs-docs-section">
        <div class="row">
            <div class="col-lg-12">
                <div class="bs-component">
                    <table class="table table-striped table-hover">
                        <thead>
                            <tr>
                                <th>Digit</th>
                                <th>Blackness Ratio</th>
                                <th>Aspect Ratio</th>
                                <th>Holes</th>
                                <th>Top Blackness Ratio</th>
                                <th>Bottom Blackness Ratio</th>
                                <th>Left Blackness Ratio</th>
                                <th>Right Blackness Ratio</th>
                                <th>Horizontal Line Ratio</th>
                                <th>Vertical Line Ratio</th>
                            </tr>
                        </thead>
                        <tbody>
<tr>
<td>0</td>
<td>0.3 5%<br>0.4 25%<br>0.5 28%<br>0.6 24%<br>0.7 10%<br>0.8 5%<br>0.9 2%<br></td>
<td>0.5 1%<br>0.6 41%<br>0.7 43%<br>0.8 9%<br>0.9 4%<br>1.0 2%<br></td>
<td>0 4%<br>1 96%<br></td>
<td>0.3 6%<br>0.4 23%<br>0.5 28%<br>0.6 26%<br>0.7 11%<br>0.8 5%<br>0.9 1%<br></td>
<td>0.3 6%<br>0.4 25%<br>0.5 28%<br>0.6 22%<br>0.7 13%<br>0.8 4%<br>0.9 1%<br></td>
<td>0.3 6%<br>0.4 25%<br>0.5 25%<br>0.6 25%<br>0.7 14%<br>0.8 2%<br>0.9 3%<br></td>
<td>0.3 7%<br>0.4 20%<br>0.5 32%<br>0.6 28%<br>0.7 6%<br>0.8 6%<br>0.9 1%<br></td>
<td>0.4 24%<br>0.5 10%<br>0.6 18%<br>0.7 17%<br>0.8 15%<br>0.9 8%<br>1.0 8%<br></td>
<td>0.6 4%<br>0.7 18%<br>0.8 25%<br>0.9 28%<br>1.0 24%<br></td>
</tr>
<tr>
<td>1</td>
<td>0.2 7%<br>0.3 20%<br>0.4 34%<br>0.5 23%<br>0.6 7%<br>0.8 5%<br>0.9 2%<br>1.0 2%<br></td>
<td>0.2 2%<br>0.3 4%<br>0.4 23%<br>0.5 24%<br>0.6 26%<br>0.7 13%<br>0.8 6%<br>0.9 2%<br></td>
<td>0 100%<br></td>
<td>0.2 11%<br>0.3 18%<br>0.4 31%<br>0.5 16%<br>0.6 10%<br>0.7 5%<br>0.8 2%<br>0.9 5%<br>1.0 2%<br></td>
<td>0.2 5%<br>0.3 23%<br>0.4 30%<br>0.5 28%<br>0.6 4%<br>0.7 3%<br>0.8 3%<br>0.9 1%<br>1.0 2%<br></td>
<td>0.1 12%<br>0.2 27%<br>0.3 18%<br>0.4 16%<br>0.5 15%<br>0.6 7%<br>0.7 2%<br>0.8 1%<br>1.0 2%<br></td>
<td>0.2 10%<br>0.3 9%<br>0.4 24%<br>0.5 17%<br>0.6 15%<br>0.7 4%<br>0.8 5%<br>0.9 2%<br>1.0 14%<br></td>
<td>0.3 2%<br>0.4 8%<br>0.6 2%<br>0.7 6%<br>0.8 15%<br>0.9 15%<br>1.0 52%<br></td>
<td>0.3 4%<br>0.4 2%<br>0.5 8%<br>0.6 7%<br>0.8 5%<br>0.9 5%<br>1.0 69%<br></td>
</tr>
<tr>
<td>2</td>
<td>0.2 5%<br>0.3 24%<br>0.4 20%<br>0.5 28%<br>0.6 17%<br>0.7 6%<br></td>
<td>0.5 2%<br>0.6 21%<br>0.7 47%<br>0.8 12%<br>0.9 6%<br>1.0 3%<br>1.1 5%<br>1.3 4%<br></td>
<td>0 100%<br></td>
<td>0.2 11%<br>0.3 18%<br>0.4 24%<br>0.5 31%<br>0.6 13%<br>0.7 3%<br></td>
<td>0.2 1%<br>0.3 23%<br>0.4 24%<br>0.5 25%<br>0.6 17%<br>0.7 8%<br>0.8 2%<br></td>
<td>0.2 22%<br>0.3 21%<br>0.4 25%<br>0.5 22%<br>0.6 9%<br>0.7 1%<br></td>
<td>0.3 11%<br>0.4 25%<br>0.5 19%<br>0.6 28%<br>0.7 13%<br>0.8 4%<br></td>
<td>0.4 4%<br>0.5 4%<br>0.6 5%<br>0.7 4%<br>0.8 16%<br>0.9 31%<br>1.0 36%<br></td>
<td>0.2 1%<br>0.3 11%<br>0.4 6%<br>0.5 37%<br>0.6 30%<br>0.7 11%<br>0.9 1%<br>1.0 3%<br></td>
</tr>
<tr>
<td>3</td>
<td>0.2 4%<br>0.3 23%<br>0.4 24%<br>0.5 26%<br>0.6 18%<br>0.7 5%<br></td>
<td>0.5 1%<br>0.6 39%<br>0.7 37%<br>0.8 13%<br>0.9 5%<br>1.0 2%<br>1.1 3%<br></td>
<td>0 100%<br></td>
<td>0.3 19%<br>0.4 31%<br>0.5 31%<br>0.6 11%<br>0.7 7%<br>0.8 1%<br></td>
<td>0.1 1%<br>0.2 8%<br>0.3 23%<br>0.4 21%<br>0.5 22%<br>0.6 22%<br>0.7 3%<br></td>
<td>0.1 3%<br>0.2 30%<br>0.3 28%<br>0.4 20%<br>0.5 16%<br>0.6 3%<br></td>
<td>0.3 5%<br>0.4 14%<br>0.5 27%<br>0.6 25%<br>0.7 22%<br>0.8 6%<br>0.9 1%<br></td>
<td>0.4 4%<br>0.5 8%<br>0.6 17%<br>0.7 13%<br>0.8 26%<br>0.9 22%<br>1.0 10%<br></td>
<td>0.3 4%<br>0.4 11%<br>0.5 15%<br>0.6 15%<br>0.7 6%<br>0.8 6%<br>0.9 26%<br>1.0 17%<br></td>
</tr>
<tr>
<td>4</td>
<td>0.3 22%<br>0.4 34%<br>0.5 30%<br>0.6 13%<br>0.7 1%<br></td>
<td>0.6 11%<br>0.7 53%<br>0.8 24%<br>0.9 2%<br>1.0 8%<br>1.1 2%<br></td>
<td>0 6%<br>1 94%<br></td>
<td>0.2 8%<br>0.3 43%<br>0.4 25%<br>0.5 18%<br>0.6 6%<br></td>
<td>0.3 11%<br>0.4 28%<br>0.5 21%<br>0.6 31%<br>0.7 7%<br>0.9 1%<br></td>
<td>0.2 35%<br>0.3 35%<br>0.4 23%<br>0.5 7%<br></td>
<td>0.3 7%<br>0.4 15%<br>0.5 24%<br>0.6 19%<br>0.7 30%<br>0.8 5%<br></td>
<td>0.5 1%<br>0.6 1%<br>0.7 1%<br>0.8 6%<br>0.9 25%<br>1.0 66%<br></td>
<td>0.4 6%<br>0.5 6%<br>0.6 3%<br>0.7 2%<br>0.8 15%<br>0.9 3%<br>1.0 65%<br></td>
</tr>
<tr>
<td>5</td>
<td>0.2 4%<br>0.3 20%<br>0.4 19%<br>0.5 30%<br>0.6 19%<br>0.7 5%<br>0.8 3%<br></td>
<td>0.5 1%<br>0.6 27%<br>0.7 43%<br>0.8 17%<br>0.9 6%<br>1.0 2%<br>1.1 3%<br>1.2 1%<br></td>
<td>0 100%<br></td>
<td>0.3 19%<br>0.4 17%<br>0.5 23%<br>0.6 24%<br>0.7 12%<br>0.8 4%<br>0.9 1%<br></td>
<td>0.1 4%<br>0.2 12%<br>0.3 23%<br>0.4 18%<br>0.5 21%<br>0.6 17%<br>0.7 4%<br>0.8 1%<br></td>
<td>0.1 2%<br>0.2 10%<br>0.3 26%<br>0.4 14%<br>0.5 23%<br>0.6 19%<br>0.7 4%<br>0.8 2%<br></td>
<td>0.2 2%<br>0.3 15%<br>0.4 19%<br>0.5 26%<br>0.6 24%<br>0.7 11%<br>0.8 3%<br></td>
<td>0.4 2%<br>0.5 10%<br>0.6 16%<br>0.7 13%<br>0.8 23%<br>0.9 23%<br>1.0 13%<br></td>
<td>0.4 9%<br>0.5 37%<br>0.6 36%<br>0.7 16%<br>1.0 2%<br></td>
</tr>
<tr>
<td>6</td>
<td>0.3 12%<br>0.4 24%<br>0.5 25%<br>0.6 22%<br>0.7 14%<br>0.8 3%<br></td>
<td>0.6 38%<br>0.7 47%<br>0.8 11%<br>0.9 2%<br>1.0 2%<br></td>
<td>0 6%<br>1 94%<br></td>
<td>0.2 8%<br>0.3 20%<br>0.4 24%<br>0.5 18%<br>0.6 22%<br>0.7 8%<br></td>
<td>0.3 2%<br>0.4 20%<br>0.5 23%<br>0.6 31%<br>0.7 15%<br>0.8 5%<br>0.9 4%<br></td>
<td>0.3 7%<br>0.4 22%<br>0.5 19%<br>0.6 30%<br>0.7 17%<br>0.8 5%<br></td>
<td>0.2 1%<br>0.3 20%<br>0.4 22%<br>0.5 22%<br>0.6 23%<br>0.7 10%<br>0.8 2%<br></td>
<td>0.4 1%<br>0.5 3%<br>0.6 7%<br>0.7 20%<br>0.8 22%<br>0.9 32%<br>1.0 15%<br></td>
<td>0.3 4%<br>0.4 4%<br>0.5 7%<br>0.6 9%<br>0.7 23%<br>0.8 25%<br>0.9 19%<br>1.0 9%<br></td>
</tr>
<tr>
<td>7</td>
<td>0.2 18%<br>0.3 36%<br>0.4 26%<br>0.5 17%<br>0.6 3%<br></td>
<td>0.5 3%<br>0.6 26%<br>0.7 48%<br>0.8 18%<br>0.9 5%<br></td>
<td>0 100%<br></td>
<td>0.3 19%<br>0.4 22%<br>0.5 26%<br>0.6 25%<br>0.7 7%<br>0.8 1%<br></td>
<td>0.1 15%<br>0.2 43%<br>0.3 33%<br>0.4 8%<br>0.5 1%<br></td>
<td>0.1 4%<br>0.2 20%<br>0.3 41%<br>0.4 20%<br>0.5 13%<br>0.6 2%<br></td>
<td>0.2 23%<br>0.3 26%<br>0.4 23%<br>0.5 13%<br>0.6 10%<br>0.7 5%<br></td>
<td>0.6 5%<br>0.7 1%<br>0.8 12%<br>0.9 33%<br>1.0 49%<br></td>
<td>0.2 3%<br>0.3 17%<br>0.4 25%<br>0.5 21%<br>0.6 8%<br>0.7 7%<br>0.8 4%<br>0.9 10%<br>1.0 5%<br></td>
</tr>
<tr>
<td>8</td>
<td>0.3 7%<br>0.4 19%<br>0.5 28%<br>0.6 19%<br>0.7 14%<br>0.8 9%<br>0.9 3%<br></td>
<td>0.5 2%<br>0.6 30%<br>0.7 41%<br>0.8 17%<br>0.9 6%<br>1.1 2%<br>1.2 2%<br></td>
<td>0 4%<br>1 3%<br>2 93%<br></td>
<td>0.3 5%<br>0.4 17%<br>0.5 33%<br>0.6 22%<br>0.7 12%<br>0.8 5%<br>0.9 5%<br>1.0 1%<br></td>
<td>0.2 3%<br>0.3 8%<br>0.4 15%<br>0.5 30%<br>0.6 17%<br>0.7 14%<br>0.8 12%<br>0.9 1%<br></td>
<td>0.3 10%<br>0.4 17%<br>0.5 30%<br>0.6 19%<br>0.7 9%<br>0.8 13%<br>0.9 2%<br></td>
<td>0.2 4%<br>0.3 6%<br>0.4 18%<br>0.5 22%<br>0.6 27%<br>0.7 11%<br>0.8 6%<br>0.9 6%<br></td>
<td>0.4 2%<br>0.5 11%<br>0.6 17%<br>0.7 15%<br>0.8 26%<br>0.9 17%<br>1.0 12%<br></td>
<td>0.3 1%<br>0.4 16%<br>0.5 11%<br>0.6 22%<br>0.7 3%<br>0.8 8%<br>0.9 19%<br>1.0 20%<br></td>
</tr>
<tr>
<td>9</td>
<td>0.3 10%<br>0.4 22%<br>0.5 30%<br>0.6 21%<br>0.7 14%<br>0.8 3%<br></td>
<td>0.6 37%<br>0.7 49%<br>0.8 9%<br>0.9 3%<br>1.0 2%<br></td>
<td>0 4%<br>1 93%<br>2 3%<br></td>
<td>0.3 5%<br>0.4 14%<br>0.5 23%<br>0.6 34%<br>0.7 14%<br>0.8 9%<br>0.9 1%<br></td>
<td>0.2 7%<br>0.3 18%<br>0.4 25%<br>0.5 17%<br>0.6 22%<br>0.7 9%<br>0.8 2%<br></td>
<td>0.2 1%<br>0.3 16%<br>0.4 20%<br>0.5 28%<br>0.6 23%<br>0.7 9%<br>0.8 2%<br></td>
<td>0.2 1%<br>0.3 7%<br>0.4 19%<br>0.5 21%<br>0.6 28%<br>0.7 18%<br>0.8 5%<br>0.9 1%<br></td>
<td>0.5 2%<br>0.6 13%<br>0.7 12%<br>0.8 27%<br>0.9 37%<br>1.0 9%<br></td>
<td>0.4 3%<br>0.5 6%<br>0.6 14%<br>0.7 19%<br>0.8 33%<br>0.9 16%<br>1.0 9%<br></td>
</tr>                        
                                          
                        </tbody>
                    </table>
                </div>
            </div>
        </div>
    </div>



    <div class="row">
        <div class="col-lg-8">
            <p>
            The table above has a lot of information but it seems pretty clear 
            that none of the features, besides the number of holes, are very 
            good at characterizing the digits. Even within a digit, the values 
            for many of the features are spread across a wide range.
            </p>
            <p>
            I think that improving the feature set is the first place that 
            I should be spending more time to improve the performance of any 
            classifier on this data set. I would try normalizing the sizes of 
            the images of the training samples, but most of the features are 
            ratios anyway so I'm not sure how much that will help. Also, it's 
            clear that the number of holes features discriminates between 
            digits better than any other feature, so scaling the feature to 
            larger values will make sure that only samples with the same number of
            holes can possibly be near each other, at least while using the 
            Euclidean distance. This would introduce some guaranteed error 
            because, as you can see from the table above, the measurement of 
            the number of holes isn't perfect, but it could increase the 
            accuracy in some cases.
            </p>
            <p>
            This feature set is probably not good enough to get good results on 
            a larger set of characters like the English alphabet. I'm not 
            confident that it's good enough for even the problem of 
            classifying the digits 0 to 9. Considerable changes will need to 
            be made first before attempting to scale up any classifier 
            built using these features.
            </p>
        </div>
    </div>
</div>

<div class="container">
    <div class="row">
        <div class="col-lg-8">
            <h1>Classifier Design</h1>
            <p>
            The first, and most successful, algorithm I tried was my own 
            implementation of k-nearest-neighbours.
            My implementation is naive in that it doesn't use any complicated
            data structures. It simply finds the distance between the given 
            record and all the training records using the Euclidean distance,
            then sorts the distances and chooses the k smallest points. 
            These points vote to classify the record, with a tie going to the 
            class with the smallest numerical value. 
            </p>
            <p>
            This implementation could be improved considerably, both in run time
            and accuracy. To improve runtime, a data structure like a 
            kd-tree could be created as a preprocessing step, then each 
            query could be done in O(n*log m) time instead of the O(nm*log m) time
            that it currently takes, where n and m are the number of test records 
            and the number of training samples, respectively.
            </p>
            <p>
            The k-nearest-neighbour classifier doesn't have very many knobs 
            to turn when trying to improve the accuracy. Using a different 
            distance function could possibly improve the results but the 
            best way for the classifer to be more accurate would be to 
            improve the set of features that are measured. I think that 
            increasing the size of the feature space, hopefully with 
            features that have some discrimination power, would help 
            spread out the classes. It seems like right now my feature 
            space is too small for the number of classes in the problem.
            </p>
            <p>
            For comparison, I also tested my dataset with a few of the 
            classifiers built into scikit-learn. Using different 
            flags when running the script from the command line, 
            you can run a random forest, naive Bayes, or support 
            vector machine on the data. I spent a little bit of time 
            tweaking each of these, but none of them were able to 
            improve on the k-nearest-neighbour classifier.
            </p>
        </div>
    </div>
</div>

<div class="container">
    <div class="row">
        <div class="col-lg-8">
            <h1>Results</h1>
            After running the k-nearest-neighbour algorithm with several choices for k, 
            it turned out that for this test set a k value of 1 did the best. The 
            following table gives the results broken down by digit.
        </div>
    </div>
     <div class="bs-docs-section">
        <div class="row">
            <div class="col-lg-12">
                <div class="bs-component">
                    <table class="table table-striped table-hover">
                        <thead>
                            <tr>
                                <th>Digit</th>
                                <th>Recognition Rate</th>
                            </tr>
                        </thead>
                        <tbody>
                        <tr> <td>0</td> <td>0.567</th> </tr>
                        <tr> <td>1</td> <td>0.631</th> </tr>
                        <tr> <td>2</td> <td>0.698</th> </tr>
                        <tr> <td>3</td> <td>0.783</th> </tr>
                        <tr> <td>4</td> <td>0.498</th> </tr>
                        <tr> <td>5</td> <td>0.660</th> </tr>
                        <tr> <td>6</td> <td>0.650</th> </tr>
                        <tr> <td>7</td> <td>0.911</th> </tr>
                        <tr> <td>8</td> <td>0.542</th> </tr>
                        <tr> <td>9</td> <td>0.512</th> </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </div>
    </div>

    <div class="row">
        <div class="col-lg-8">
            <p>
            The overall correct classification rate was around 0.643. The overall 
            rate is lower than I had hoped but I think it could 
            be improved significantly by putting some more work into the 
            feature set. 
            </p>
            <p>
            There are some surprises here in the per digit rates that 
            I'm not really sure how to explain. It seems that the 
            digits with no holes are easier to classify than the digits 
            with one or more holes, because you can see that the highest 
            correct classification rate was for the digits 7, 3 and 2, with 
            5 and 1 also doing relatively well. 
            </p>
            <p>
            More surprising to me is that the only digit with two holes, 
            namely 8, was one of the most difficult to classify. I expect 
            that scaling the number of holes feature to make it more 
            significant than the other features would change that, but I 
            still expected it to be one of the easier digits to classify 
            without having to go to such lengths.
            </p>
            <p>
            The other classifiers did worse than the nearest-neighbour 
            algorithm that I implemented myself. The random forest 
            classifier did best with a forest of 100 trees and ended up 
            with a recognition rate of about 0.621.
            </p>
            <p>
            The naive Bayes classifier assumed a Gaussian distribution 
            of the data, and came up with a 0.531 recognition rate.
            </p>
            <p>
            Finally, I tried using a support vector machine classifier 
            on my data set with a linear kernel function and got a 
            0.611 recognition rate. This was closest to the 
            nearest-neighbour classifier but after examining the 
            per digit recognition rates, they were down nearly across the 
            board so no benefit is gained from using the SVM.
        </div>
    </div>
</div>

<div class="container">
    <div class="row">
        <div class="col-lg-8">
            <h1>Source Code</h1>
            <p>
            The complete source code for the classifier can be found at
            <a href="http://github.com/rgcase/digitclassifierproject">
            http://github.com/rgcase/digitclassifierproject</a>. Included
            are some text files 'trainingdata.txt' and 'testdata.txt' 
            that have all the data I extracted from the training and 
            test images.
        </div>
    </div>
</div>



<div class="container">
    <div class="row">
        <div class="col-lg-8">
            <h1>References</h1>
            <p>
            [1]Pillow: <a href="https://pillow.readthedocs.org/">https://pillow.readthedocs.org/</a></br>
            [2]scikit-learn: <a href="http://scikit-learn.org/">http://scikit-learn.org/</a>
            </p>
        </div>
    </div>
</div>

</body>
