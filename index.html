<!DOCTYPE html>
<html lang="en">
<head>
    <title>Digit Classifier</title>
    <link rel="stylesheet" href="./css/bootstrap.min.css">
</head>

<body>
<div class="navbar navbar-default navbar-fixed-top">
    <div class="container">
        <div class="navbar-header">
            <div class="navbar-brand">Digit Classifier</div>
        </div>
    </div>
</div>

<div class="container">
    <div class="page-header" id="banner">
        <div class="row">
            <div class="col-lg-8 col-md-7 col-sm-6">
                <h1>CISC 859 Digit Classifier Project</h1></br>
                <h3>Ryan Case, 10071517</h3>
            </div>
        </div>
    </div>
</div>

<div class="container">
    <div class="row">
        <div class="col-lg-8">
            <h1>Introduction</h1>
            <p>
            This is a classifier for numerical digits 0-9 that was completed as a project 
            for CISC 859 at Queen's University.
            </p>
            <p>
            Image files are accepted in .bmp format for both training and classification,
            and are dealt with using the Pillow[1] library.
            </p>
            <p>
            Classification methods include a naive implementation of k-nearest neighbours,
            and multiple other methods that are available in the scikit-learn[2] library.
            </p>
            <p>
            The accuracy of the classifier is not really acceptable for any real use but 
            it does a better job than random guessing. The framework is here though, so 
            it could be improved by doing better feature selection and tweaking the 
            classifiers that are used.
            </p>
            <p>
            The source code (along with this website) are hosted on github. You can find it
            <a href="http://github.com/rgcase/digitclassifierproject">here</a>.
            </p>
        </div>
    </div>
</div>

<hr>

<div class="container">
    <div class="row">
        <div class="col-lg-8">
            <h1>Feature Extraction</h1>
            <p>
            The examples I used for both the training and test data were obtained from
            the CISC 859 course website. These images can be found 
            <a href="http://research.cs.queensu.ca/home/cisc859/download/859.digits/bmp.images.patrice/">
            here</a>. The images used for training were the files prefixed by "trg" and 
            the test images were the files prefixed by "test".
            </p>
            <p>
            The first phase of extracting features is to extract the digits themselves
            from the .bmp images. This happens in <a href="https://github.com/rgcase/digitclassifierproject/blob/master/partitionimages.py">
            partitionimages.py</a>. Each image file is opened and loaded as a greyscale 
            Image object using Pillow.
            </p>
            <p>
            Each file is thresholded so that pixels with value less than 200 are set to
            0 and pixels with value greater than 200 are set to 255. After several 
            tests, this threshold value seemed to produce the best results when 
            extracting the individual digits. This extraction is done using a region
            growing algorithm that can be found in 
            <a href="https://github.com/rgcase/digitclassifierproject/blob/master/imageprocessing.py">
            imageprocessing.py</a>. Once a black region is found, the bounding box
            is computed, then a new Image object is created containing everything
            in that bounding box. The image is saved in a folder containing all the 
            images of the same digit, separated into training and test set.
            </p>
            <p>
            Once the digits have all been extracted, the feature extraction can begin. 
            All of this is accomplished in the 
            <a href="https://github.com/rgcase/digitclassifierproject/blob/master/extractfeatures.py">
            extractfeatures.py</a> file. The features that I used are:
            </p>
            <ul>
                <li>Blackness Ratio</li>
                <li>Aspect Ratio of the Bounding Box</li>
                <li>Number of Holes</li>
                <li>Half-Blackness Ratio Top, Bottom, Left, Right)</li>
                <li>Ratio of Longest Horizontal Line to Horizontal Width</li>
                <li>Ratio of Longest Vertical Line to Vertical Width</li>
            </ul>
            <p>
            All this information is written to a .csv file so it can be used later
            by the classification algorithms.
            </p>
            <p>
            In the following table, for each digit the values of each measured 
            feature are shown rounded to one decimal place, with the percentage 
            of that digit having the corresponding feature value next to it. The 
            table includes the data from the entire training set.
            </p>
        </div>
    </div>
    
    <div class="bs-docs-section">
        <div class="row">
            <div class="col-lg-12">
                <div class="bs-component">
                    <table class="table table-striped table-hover">
                        <thead>
                            <tr>
                                <th>Digit</th>
                                <th>Blackness Ratio</th>
                                <th>Aspect Ratio</th>
                                <th>Holes</th>
                                <th>Top Blackness Ratio</th>
                                <th>Bottom Blackness Ratio</th>
                                <th>Left Blackness Ratio</th>
                                <th>Right Blackness Ratio</th>
                                <th>Horizontal Line Ratio</th>
                                <th>Vertical Line Ratio</th>
                            </tr>
                        </thead>
                        <tbody>
                        <tr>
                            <td>0</td>
                            <td>0.3 5%<br>0.4 25%<br>0.5 28%<br>0.6 24%<br>0.7 10%<br>0.8 5%<br>0.9 2%</td>
                            <td>0.5 1%<br>0.6 41%<br>0.7 43%<br>0.8 9%<br>0.9 4%<br>1.0 2%</td>
                            <td>0 4%<br>1 96%</td>
                            <td>0.3 6%<br>0.4 23%<br>0.5 28%<br>0.6 26%<br>0.7 11%<br>0.8 5%<br> 0.9 1%</td>
                            <td>
                        </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </div>
    </div>



    <div class="row">
        <div class="col-lg-8">
            <p>
            This feature set is probably not good enough to get good results on 
            a larger set of characters like the English alphabet. I didn't do 
            any rescaling of the digits which might help get better results. 
            I also think that tweaking the feature space so that each feature 
            contributes a roughly equal amount to a distance calculation 
            could help improve accuracy. 
            </p>
        </div>
    </div>
</div>

<div class="container">
    <div class="row">
        <div class="col-lg-8">
            <h1>Classifier Design</h1>
            <p>
            The first, and most successful, algorithm I tried was my own 
            implementation of k-Nearest-Neighbours.
            My implementation is naive in that it doesn't use any complicated
            data structures. It simply finds the distance between the given 
            record and all the training records using the Euclidean distance,
            then sorts the distances and chooses the k smallest points. 
            These points vote to classify the record, with a tie going to the 
            class with the smallest numerical value. 
            </p>
            <p>
            This implementation could be improved considerably, both in run time
            and accuracy. To improve runtime, a data structure like a 
            kd-tree could be created as a preprocessing step, then each 
            query could be done in O(n*log m) time instead of the O(nm*log m) time
            that it currently takes, where n and m are the number of test records 
            and the number of training samples, respectively.
            </p>
            <p>
            The k-Nearest-Neighbour classifier doesn't have very many knobs 
            to turn when trying to improve the accuracy. Using a different 
            distance function could possibly improve the results but the 
            best way for the classifer to be more accurate would be to 
            improve the set of features that are measured. I think that 
            increasing the size of the feature space, hopefully with 
            features that have some discrimination power, would help 
            spread out the classes. It seems like right now my feature 
            space is too small for the number of classes in the problem.
            </p>
            <p>
            For comparison


        </div>
    </div>
</div>




<div class="container">
    <div class="row">
        <div class="col-lg-8">
            <h1>References</h1>
            <p>
            [1]Pillow: <a href="https://pillow.readthedocs.org/">https://pillow.readthedocs.org/</a></br>
            [2]scikit-learn: <a href="http://scikit-learn.org/">http://scikit-learn.org/</a>
            </p>
        </div>
    </div>
</div>

</body>
